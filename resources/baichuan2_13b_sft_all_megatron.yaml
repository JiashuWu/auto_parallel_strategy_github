# model
model_path: ${MODEL_DIR}

# train
do_train: true
num_train_epochs: ${EPOCHS}
bf16: true
per_device_train_batch_size: &micro_batch_size_ref ${MICRO_BATCH_SIZE}
learning_rate: &lr_ref ${LEARNING_RATE}
lr_scheduler_type: &lr_scheduler_type_ref cosine
# train - sft
finetuning_type: full

# dataset
train_dataset: train_data
dataset_dir: ${DATASET_PATH}
val_ratio: 0.1
# max_samples: 1000
# dataset - sft
template: baichuan2
ignore_pad_token_for_loss: true
max_length: &seq_length_ref 128
data_variable_seq_lengths: &variable_lengths_ref false
packing: false

# checkpoint
output_dir: ${ORIGINAL_OUTPUT_DIR}
save_steps: 100000
max_steps: 20
save_total_limit: 2
save_strategy: 'no'

# output
logging_steps: 1

# convert_ckpt
megatron_model_flag: true # 是否使用megatron格式模型
model_type: GPT
tokenizer_model: ${MODEL_DIR}/tokenizer.model
model_type_hf: baichuan2

# megatron
megatron_sft: true
megatron_dataset_flag: false # 是否使用megatron格式数据
megatron_log_throughput: true # 是否打印吞吐量日志
megatron_lm:
    seed: 42
    # accelerate
    model_type_name: "gpt"
    orig_vocab_size: 32000
    model_return_dict: true
    pretraining_flag: false
    return_logits: false
    # megatron
    data_path:
        - megatron/dataset
    split: "100,0,0"
    load: ${TRANS_MODEL_BASE_PATH}
    use_mcore_models: true
    reuse_fp32_param: false
    overlap_grad_reduce: SUBSTITUTION_OP
    overlap_param_gather: SUBSTITUTION_OP
    tensor_model_parallel_size: SUBSTITUTION_TP
    pipeline_model_parallel_size: SUBSTITUTION_PP
    sequence_parallel: SUBSTITUTION_SP
    use_distributed_optimizer: SUBSTITUTION_OP
    #recompute_activation_function: true
    #recompute_method: "block"
    #recompute_granularity: "full"
    #recompute_num_layers: 40
    num_layers: 40
    hidden_size: 5120
    ffn_hidden_size: 13696
    num_attention_heads: 40
    seq_length: *seq_length_ref
    add_bias_linear: false
    disable_bias_linear: true
    max_position_embeddings: 8192
    micro_batch_size: *micro_batch_size_ref
    global_batch_size: ${GLOBAL_BATCH_SIZE}
    untie_embeddings_and_output_weights: true
    gradient_accumulation_fusion: false
    make_vocab_size_divisible_by: 32
    use_flash_attn: true
    lr: *lr_ref
    min_lr: 1.25e-7
    lr_decay_style: *lr_scheduler_type_ref
    attention_dropout: 0.0
    init_method_std: 0.01
    position_embedding_type: alibi
    hidden_dropout: 0.0
    norm_epsilon: 1.0e-6
    normalization: RMSNorm
    use_fused_rmsnorm: true
    use_fused_swiglu: true
    swiglu: true
    masked_softmax_fusion: false
    attention_softmax_in_fp32: false
    clip_grad: 1.0
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_eps: 1.0e-8
    initial_loss_scale: 1
    weight_decay: 1.0e-1
    rotary_base: 1000000
    bf16: true
    tokenizer_type: "PretrainedFromHF"
    tokenizer_name_or_path: ${MODEL_DIR}
    tokenizer_model: ${MODEL_DIR}/tokenizer.model
    tokenizer_not_use_fast: true
    tokenizer_padding_side: right
    variable_seq_lengths: *variable_lengths_ref
    # finetune_args
    stage: sft
    finetune: true
    is_instruction_dataset: true
    prompt_type: baichuan2
