# model
model_path: ${MODEL_DIR}

# train
do_train: true
num_train_epochs: ${EPOCHS}
bf16: true
per_device_train_batch_size: ${MICRO_BATCH_SIZE}
learning_rate: &lr_ref ${LEARNING_RATE}
lr_scheduler_type: &lr_scheduler_type_ref cosine
# train - sft
finetuning_type: full

# dataset
train_dataset: train_data
dataset_dir: ${DATASET_PATH}
val_ratio: 0.1
# max_samples: 1000
# dataset - sft
template: llama2
ignore_pad_token_for_loss: true
max_length: &seq_length_ref 128
data_variable_seq_lengths: &variable_lengths_ref false
packing: false

# checkpoint
output_dir: ${ORIGINAL_OUTPUT_DIR}
save_steps: 100000
max_steps: 20
save_total_limit: 2
save_strategy: 'no'

# output
logging_steps: 1

# convert_ckpt
megatron_model_flag: true # 是否使用megatron格式模型
model_type: GPT
tokenizer_model: ${MODEL_DIR}/tokenizer.model
model_type_hf: llama2

# megatron
megatron_sft: true
megatron_dataset_flag: false # 是否使用megatron格式数据
megatron_log_throughput: true # 是否打印吞吐量日志
megatron_lm:
    # accelerate
    model_type_name: "gpt"
    orig_vocab_size: 32000
    model_return_dict: true
    pretraining_flag: false
    return_logits: false
    # megatron
    data_path:
        - megatron/dataset
    split: "100,0,0"
    load: ${TRANS_MODEL_BASE_PATH}
    use_mcore_models: true
    tensor_model_parallel_size: SUBSTITUTION_TP
    pipeline_model_parallel_size: SUBSTITUTION_PP
    use_distributed_optimizer: SUBSTITUTION_OP
    sequence_parallel: SUBSTITUTION_SP
    #context_parallel_size: 1
    #context_parallel_algo: "megatron_cp_algo"
    num_layers: 32
    hidden_size: 4096
    ffn_hidden_size: 11008
    num_attention_heads: 32
    tokenizer_type: PretrainedFromHF
    tokenizer_name_or_path: ${MODEL_DIR}
    tokenizer_not_use_fast: true
    variable_seq_lengths: *variable_lengths_ref
    prompt_type: llama2
    seq_length: *seq_length_ref
    max_position_embeddings: *seq_length_ref
    # default 4
    micro_batch_size: ${MICRO_BATCH_SIZE}
    global_batch_size: ${GLOBAL_BATCH_SIZE}
    make_vocab_size_divisible_by: 1
    lr: *lr_ref
    lr_decay_style: *lr_scheduler_type_ref
    untie_embeddings_and_output_weights: true
    # dest
    add_bias_linear: false
    attention_dropout: 0.0
    init_method_std: 0.01
    hidden_dropout: 0.0
    position_embedding_type: "rope"
    normalization: "RMSNorm"
    use_fused_rmsnorm: true
    swiglu: true
    use_flash_attn: true
    # dest
    masked_softmax_fusion: false
    attention_softmax_in_fp32: false
    min_lr: 0.0
    weight_decay: 1.0e-1
    lr_warmup_fraction: 0.1
    clip_grad: 1.0
    adam_beta1: 0.9
    initial_loss_scale: 1
    adam_beta2: 0.95
    # dest
    gradient_accumulation_fusion: false
    no_load_optim: true
    no_load_rng: true
    bf16: true
    # tune_args
    stage: sft
    is_instruction_dataset: true
