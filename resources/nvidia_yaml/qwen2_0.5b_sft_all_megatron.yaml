# model
model_path: ${MODEL_DIR}

# train
do_train: true
num_train_epochs: ${EPOCHS}
bf16: true
per_device_train_batch_size: ${MICRO_BATCH_SIZE}
learning_rate: &lr_ref ${LEARNING_RATE}
lr_scheduler_type: &lr_scheduler_type_ref cosine
# train - sft
finetuning_type: full

# dataset
train_dataset: train_data
dataset_dir: ${DATASET_PATH}
val_ratio: 0.1
# max_samples: 1000
# dataset - sft
template: qwen
ignore_pad_token_for_loss: true
max_length: &seq_length_ref 128
data_variable_seq_lengths: &variable_lengths_ref false
packing: false

# checkpoint
output_dir: ${ORIGINAL_OUTPUT_DIR}
save_steps: 100000
save_total_limit: 2
max_steps: 20
save_strategy: 'no'

# output
logging_steps: 1

# convert_ckpt
megatron_model_flag: true # 是否使用megatron格式模型
model_type: GPT
#tokenizer_model: ${MODEL_DIR}/tokenizer.json
model_type_hf: llama2
params_dtype: bf16

# megatron
megatron_sft: true
megatron_dataset_flag: false # 是否使用megatron格式数据
megatron_log_throughput: true # 是否打印吞吐量日志
megatron_lm:
    # accelerate
    model_type_name: "gpt"
    orig_vocab_size: 32000
    model_return_dict: true
    pretraining_flag: false
    return_logits: false
    # megatron
    data_path:
        - megatron/dataset
    split: "100,0,0"
    load: ${TRANS_MODEL_BASE_PATH}
    use_mcore_models: true
    tensor_model_parallel_size: SUBSTITUTION_TP
    pipeline_model_parallel_size: SUBSTITUTION_PP
    sequence_parallel: SUBSTITUTION_SP
    use_distributed_optimizer: SUBSTITUTION_OP
    num_layers: 24
    hidden_size: 896
    ffn_hidden_size: 4864
    num_attention_heads: 14
    group_query_attention: true
    num_query_groups: 2
    #tokenizer_type: PretrainedFromHF
    #tokenizer_name_or_path: ${MODEL_DIR}
    tokenizer_type: "HuggingFaceTokenizer"
    tokenizer_name_or_path: ${MODEL DIR}
    tokenizer_model: ${MODEL_DIR}
    max_position_embeddings: *seq_length_ref
    seq_length: *seq_length_ref
    micro_batch_size: ${MICRO_BATCH_SIZE}
    global_batch_size: ${GLOBAL_BATCH_SIZE}
    make_vocab_size_divisible_by: 1
    padded_vocab_size: 151936
    rotary_base: 1000000
    lr: *lr_ref
    lr_decay_style: *lr_scheduler_type_ref
    add_qkv_bias: true
    # dest
    add_bias_linear: false
    attention_dropout: 0.0
    init_method_std: 0.01
    hidden_dropout: 0.0
    position_embedding_type: rope
    normalization: RMSNorm
    norm_epsilon: 1.0e-6
    swiglu: true
    use_fused_swiglu: true
    use_flash_attn: true
    use_fused_rotary_pos_emb: true
    use_rotary_position_embeddings: true
    use_fused_rmsnorm: true
    # dest
    masked_softmax_fusion: false
    attention_softmax_in_fp32: true
    min_lr: 0.0
    weight_decay: 1.0e-1
    lr_warmup_fraction: 0.01
    clip_grad: 1.0
    adam_beta1: 0.9
    adam_beta2: 0.95
    # dest
    gradient_accumulation_fusion: false
    no_load_optim: true
    no_load_rng: true
    initial_loss_scale: 4096
    overlap_grad_reduce: SUBSTITUTION_OP
    seed: 42
    bf16: true
    # tune_args
    stage: sft
    is_instruction_dataset: true
    variable_seq_lengths: *variable_lengths_ref
    tokenizer_not_use_fast: true
    prompt_type: qwen
